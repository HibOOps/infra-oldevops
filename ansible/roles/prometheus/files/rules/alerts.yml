groups:
  - name: infrastructure_alerts
    interval: 30s
    rules:
      # Service availability alerts
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} on {{ $labels.instance }} has been down for more than 1 minute."

      - alert: ServiceFlapping
        expr: changes(up[5m]) > 3
        for: 2m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Service {{ $labels.job }} is flapping"
          description: "{{ $labels.job }} on {{ $labels.instance }} has restarted {{ $value }} times in the last 5 minutes."

      # Resource utilization alerts
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          component: resource
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 80% (current value: {{ $value | humanize }}%)"

      - alert: CriticalCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 2m
        labels:
          severity: critical
          component: resource
        annotations:
          summary: "Critical CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 95% (current value: {{ $value | humanize }}%)"

      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          component: resource
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 85% (current value: {{ $value | humanize }}%)"

      - alert: CriticalMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 95
        for: 2m
        labels:
          severity: critical
          component: resource
        annotations:
          summary: "Critical memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 95% (current value: {{ $value | humanize }}%)"

      # Disk space alerts
      - alert: DiskSpaceLow
        expr: (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"} / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs"})) * 100 > 80
        for: 5m
        labels:
          severity: warning
          component: storage
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Disk {{ $labels.mountpoint }} usage is above 80% (current value: {{ $value | humanize }}%)"

      - alert: DiskSpaceCritical
        expr: (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"} / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs"})) * 100 > 90
        for: 2m
        labels:
          severity: critical
          component: storage
        annotations:
          summary: "Critical disk space on {{ $labels.instance }}"
          description: "Disk {{ $labels.mountpoint }} usage is above 90% (current value: {{ $value | humanize }}%)"

      - alert: DiskWillFillIn4Hours
        expr: predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"}[1h], 4 * 3600) < 0
        for: 5m
        labels:
          severity: warning
          component: storage
        annotations:
          summary: "Disk will fill in 4 hours on {{ $labels.instance }}"
          description: "Disk {{ $labels.mountpoint }} is predicted to fill in 4 hours based on current growth rate."

      # Container/Docker alerts
      - alert: ContainerDown
        expr: absent(container_last_seen{name!=""})
        for: 2m
        labels:
          severity: warning
          component: container
        annotations:
          summary: "Container {{ $labels.name }} is down"
          description: "Container {{ $labels.name }} has been down for more than 2 minutes."

      - alert: ContainerHighMemory
        expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) * 100 > 90
        for: 5m
        labels:
          severity: warning
          component: container
        annotations:
          summary: "Container {{ $labels.name }} high memory usage"
          description: "Container memory usage is above 90% (current value: {{ $value | humanize }}%)"

      # Prometheus self-monitoring
      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 1m
        labels:
          severity: critical
          component: prometheus
        annotations:
          summary: "Prometheus configuration reload failed"
          description: "Prometheus configuration reload has failed. Check prometheus logs."

      - alert: PrometheusTSDBCompactionsFailing
        expr: rate(prometheus_tsdb_compactions_failed_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
          component: prometheus
        annotations:
          summary: "Prometheus TSDB compactions are failing"
          description: "Prometheus TSDB compactions are failing at rate {{ $value }}/sec."

      - alert: PrometheusTargetDown
        expr: up{job!="prometheus"} == 0
        for: 2m
        labels:
          severity: warning
          component: prometheus
        annotations:
          summary: "Prometheus target {{ $labels.job }} is down"
          description: "Prometheus cannot scrape {{ $labels.job }} on {{ $labels.instance }}."

  - name: loki_alerts
    interval: 30s
    rules:
      # Loki availability
      - alert: LokiDown
        expr: up{job="loki"} == 0
        for: 2m
        labels:
          severity: critical
          component: logging
        annotations:
          summary: "Loki is down"
          description: "Loki log aggregation system has been down for more than 2 minutes."

      # Loki ingestion rate
      - alert: LokiHighIngestionRate
        expr: rate(loki_distributor_lines_received_total[5m]) > 10000
        for: 5m
        labels:
          severity: warning
          component: logging
        annotations:
          summary: "Loki high log ingestion rate"
          description: "Loki is ingesting {{ $value }} log lines per second (may indicate logging storm)."

  - name: grafana_alerts
    interval: 30s
    rules:
      # Grafana availability
      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 2m
        labels:
          severity: warning
          component: monitoring
        annotations:
          summary: "Grafana is down"
          description: "Grafana visualization platform has been down for more than 2 minutes."
